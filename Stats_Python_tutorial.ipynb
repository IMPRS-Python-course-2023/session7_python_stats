{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics in Python\n",
    "\n",
    "*IMPRS - Using Python for Cognitive Science (2023). This tutorial is made by Noor Seijdel and based on work by [Sophie Slaats](https://www.mpi.nl/people/slaats-sophie)*\n",
    "\n",
    "Good afternoon! Today we will be working through this tutorial to get familiar with doing statistics in Python. Important note: I am by no means a statistical advisor. I will just show you today how to use a couple of packages to do some statistical tests. Basically, I provide some pointers. When doing analyses on your own data, make your own decisions on which tests to use.\n",
    "The reference guides for the packages we used in this session are here:\n",
    "\n",
    "- [Python's statsistics](https://docs.python.org/3/library/statistics.html) \n",
    "Python’s statistics is a built-in Python library for descriptive statistics. It is not intended to be a competitor to libraries such as NumPy or SciPy or full-featured statistics packages aimed at professional statistics. It is aimed at the level of graphing and scientific calculators. \n",
    "- [Numpy](https://numpy.org/doc/stable/)\n",
    "NumPy is a library for numerical computing, optimized for working with single- and multi-dimensional arrays. This library contains many routines for statistical analysis.\n",
    "- [SciPy Stats](https://docs.scipy.org/doc/scipy/reference/stats.html)\n",
    "SciPy is a library for scientific computing based on NumPy. It offers additional functionality compared to NumPy, including scipy.stats for statistical analysis.\n",
    "- [Statsmodels](https://www.statsmodels.org/stable/index.html) \n",
    "Statsmodels is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. It supports specifying models using R-style formulas and pandas DataFrames. \n",
    "- [Seaborn](https://seaborn.pydata.org/index.html) \n",
    "Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "- [Pandas](https://pandas.pydata.org/)\n",
    "Pandas is a library for numerical computing based on NumPy. It excels in handling labeled data with DataFrame objects.\n",
    "\n",
    "First, let's start again by \n",
    "1. Making a virtual environment for this week's session. For this, open a terminal in the right folder and type `python -m venv .venv`\n",
    "2. Open a new terminal (make sure you're in the virtual environment) and install the necesssary packages using  `pip install -r requirements.txt` \n",
    "3. In this notebook, select your virtual environment at \"select kernel'\n",
    "\n",
    "All done? Then you're ready to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 0. Importing modules\n",
    "Today we need `math`, `statistics`, `numpy`, `pandas` and `os`.   \n",
    "\n",
    "<font color='green'>**Exerc|ise 1:**</font> Import them below. We have added some other packages for you already.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data + descriptives\n",
    "\n",
    "Let's start by getting some data to work with. We can use the participant data from a couple weeks ago! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() #this is the path to the current working directory, if you want to use a different file: feel free to change!\n",
    "\n",
    "# load the data we used previously\n",
    "participants = pd.read_csv(os.path.join(path, 'participants.csv'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so that's the overview of our participants.. Now for each participant we have a separate file containing the data. Let's use a for loop to get all data in one structure. \n",
    "\n",
    "<font color='green'>**Exercise 2:**</font> Use a for-loop to go through the participant list and append the data from each participant to a dataframe `trials`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = pd.DataFrame()\n",
    "\n",
    "# for each participant id, load the relevant .csv file and append it to the \"trials dataframe\". \n",
    "# In the end, we want all our data (from each participant) to be stored in one dataframe. \n",
    "\n",
    "## your code here \n",
    "for participant_id in participants[##what to add here?]:\n",
    "    participant_data = pd.read_csv(##what to add here?)\n",
    "    trials = pd.concat([##what to add here?]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.head(-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've also seen before, based on their \"id\" we can merge the participant information with their data (note that we would actually rarely do this in real life, to keep the data anonymous). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge participant and trial data\n",
    "trials = trials.merge(participants, on='id')\n",
    "trials.rename(columns={'Unnamed: 0': 'trial_order'}, inplace=True)\n",
    "trials.head(-300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing data analyses, always check your data, to make sure no weird things are in there. Quite often, your data will contain NaN values (e.g. when a participant did not press the response button after a trial, or when they did not fill in a certain question etc.). Let's simulate such a situation by adding some NaN values to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_trials = np.shape(trials)[0]\n",
    "nan_indices = np.random.permutation(no_of_trials)[0:60]\n",
    "trials_NA = trials.copy()\n",
    "\n",
    "# replace values using pd.loc: \n",
    "#function allows you to access a group of rows and columns by label(s)\n",
    "trials_NA.loc[nan_indices, 'RT'] = np.nan\n",
    "trials_NA.head(-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = trials.groupby(by='condition').aggregate(\n",
    "    mean_RT=pd.NamedAgg('RT', np.mean),\n",
    "    median_RT=pd.NamedAgg('RT', np.median),\n",
    "    std_RT=pd.NamedAgg('RT', np.std),\n",
    "    mean_age=pd.NamedAgg('age', np.mean)\n",
    ")\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index (we've seen this before)\n",
    "summary.reset_index(inplace=True)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results\n",
    "Now we can use seaborn to plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we've seen in session 5:\n",
    "sns.boxplot(x='condition', y='RT', data=trials)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 3:**</font> Try to make a violinplot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If we compare these plots to the ones we made in session 5, it seems that our NaN values are dealt with appropriately (the plots are the same). Good to know: using Pandas's groupby-function handles your NaN-values for you. However, if you just want to get some means...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_stats = statistics.mean(trials_NA[\"RT\"])\n",
    "numpy_stats = np.mean(trials_NA[\"RT\"])\n",
    "\n",
    "print(python_stats, numpy_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when you start working with a new package or module, it is useful to first check how it deals with missing values. If you want to use python statistics, you should remove your NaN-values before computing your descriptive statistics. Fortunately there is a useful function for that: the Pandas `.dropna` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_dropna = trials_NA.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 4:**</font> print the mean RT of our new `trials_dropna` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 5:**</font> What is the mean age of our participants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing assumptions\n",
    "\n",
    "Let's say we'd like to perform a t-test. Then we first need to check the assumptions: \n",
    "\n",
    "1. Scale of measurement: ordinal or continuous scale\n",
    "2. Simple random sample & reasonable sample size\n",
    "3. Normal distribution of dependent variables\n",
    "4. Homogeneity of variance\n",
    "\n",
    "\n",
    "#### 2.1 Normal distribution\n",
    "\n",
    "Graphical methods: Plotting & evaluating\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Seaborn for a simple histogram\n",
    "sns.histplot(trials, x=\"RT\", hue=\"condition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.. this is not a normal distribution; it is skewed and can be best described by an exponential. But wait - the means have to be normally distributed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmeans = trials.groupby(by=['id', 'condition']).aggregate(\n",
    "    mean_RT=pd.NamedAgg('RT', np.mean),\n",
    "    std_RT=pd.NamedAgg('RT', np.std))\n",
    "\n",
    "pmeans.reset_index(inplace=True)\n",
    "pmeans.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(pmeans, x=\"mean_RT\", hue=\"condition\", kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 5:**</font> Try to plot the histogram with more bins (e.g. 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, another plot. Scipy.stats.probplot generates a probability plot of sample data against the quantiles of a specified theoretical distribution (the normal distribution by default). Probplot optionally calculates a best-fit line for the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cond in ['baseline', 'condition_a', 'condition_b']:\n",
    "    plot = stats.probplot(pmeans.loc[pmeans['condition']==cond, 'mean_RT'], plot=plt)\n",
    "    plt.title(str(cond))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'>**To think:**</font> Why can we just use `stats.probplot`? Don't we need to indicate that we want to use SciPy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these plots can be confusing. Let's try a statistical test: the Shapiro-Wilk test for normality.\n",
    "Important to keep in mind: The Shapiro-Wilk test (or another test, Kolmogorov-Smirnov) is too sensitive when sample sizes are large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple shapiro test:\n",
    "pmeans.groupby('condition').aggregate(stats.shapiro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_results = pmeans.groupby(by='condition').aggregate(\n",
    "    shapiro=pd.NamedAgg('mean_RT',stats.shapiro))\n",
    "\n",
    "shapiro_results.reset_index(inplace=True)\n",
    "shapiro_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>**Exercise 6:**</font> Look at the results (and the online documentation on the test). What do you conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performing a T-test using SciPy\n",
    "\n",
    "Okay, time to perform our first t-test to see if there are differences between our `condition_a` and the `baseline`. \n",
    "For this, we can use [scipy.stats.ttest_rel](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = trials.loc[trials['condition']=='baseline', 'RT']\n",
    "a = trials.loc[trials['condition']=='condition_a', 'RT']\n",
    "\n",
    "stats.ttest_rel(base,a, nan_policy='raise')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! So condition a seems to differ from the baseline. \n",
    "\n",
    "<font color='green'>**Exercise 7:**</font> Write a for loop that performs a t-test for both our conditions and prints the result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here \n",
    "# for cond in ['condition_a', 'condition_b']:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now - do the conditions differ from each other?\n",
    "\n",
    "<font color='green'>**Exercise 8:**</font> Perform a t-test to compare condition a and condition b: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we should correct for multiple comparisons here. We can do this, but it would be better to use a repeated measures ANOVA. For this, we need the package Statsmodels.\n",
    "\n",
    "## 4. Anova and LMM using Statsmodels\n",
    "\n",
    "statsmodels is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. An extensive list of result statistics are available for each estimator. The results are tested against existing statistical packages to ensure that they are correct.  The online documentation is hosted at [statsmodels.org](statsmodels.org).\n",
    "\n",
    "#### Repeated Measures ANOVA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.anova import AnovaRM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Statsmodels ANOVA example below we use our dataframe object, `trial`, as the first argument, followed by our dependent variable `RT`, subject identifier `id`, and the list of the independent variable, `condition`. At the end, we are getting the fit so that we can print the ANOVA table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aov = AnovaRM(\n",
    "    trials,\n",
    "    depvar='RT',\n",
    "    subject='id',\n",
    "    within=['condition'],\n",
    "    aggregate_func='mean'\n",
    ").fit()\n",
    "\n",
    "print(aov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Mixed Models\n",
    "\n",
    "In the early days, linear mixed models were not available in Python and one had save the data from Python, open up the data in R and run the model. Over the years, R & Python got to know each other a little better and several options have emerged for running LMM analyses in Python. Today we will use Statsmodels: \n",
    "\n",
    "https://www.statsmodels.org/stable/mixed_linear.html\n",
    "\n",
    "The statsmodels imputation of linear mixed models (MixedLM) closely follows the approach outlined in Lindstrom and Bates (JASA 1988). This is also the approach followed in the R package LME4. Other packages such as Stata, SAS, etc. should also be consistent with this approach, as the basic techniques in this area are mostly mature.\n",
    "\n",
    "Here we show how linear mixed models can be fit using the MixedLM procedure in statsmodels:\n",
    "\n",
    "- Formula to specify the model. Here: RT ~ condition\n",
    "- Data for the model. Here: trials\n",
    "- Re_formula: one-sided formula defining the variance structure of the model (Default = random intercept for each group). Here: 1\n",
    "- Groups: random intercept\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outcome variable is the RT, and the only predictor variable we will use here is “condition”. First we fit a model that expresses the mean RT as a function of condition, with a random intercept for each participant. The model is specified using formulas. Since the random effects structure is not specified, the default random effects structure (a random intercept for each group) is automatically used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "lmm0 = smf.mixedlm(\"RT ~ condition\", trials, groups = 'id', re_formula='1')\n",
    "lmm0f = lmm0.fit()\n",
    "print(lmm0f.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add more predictor variables by editing our formula: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmm1 = smf.mixedlm(\"RT ~ condition + age\", trials, groups = 'id', re_formula='1')\n",
    "lmm1f = lmm1.fit()\n",
    "print(lmm1f.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmm2 = smf.mixedlm(\"RT ~ condition + age + condition * age\", trials, groups = 'id', re_formula='1')\n",
    "lmm2f = lmm2.fit()\n",
    "print(lmm2f.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmm3 = smf.mixedlm(\"RT ~ condition + condition * age\", trials, groups = 'id', re_formula='1')\n",
    "lmm3f = lmm3.fit()\n",
    "print(lmm3f.summary())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>**Homework exercise**</font>\n",
    "\n",
    "This is our final session. After today, you know how to work with Python on your own laptop, to work with GIT for code organisation (and version control), to use Python's built-in data types, functions and modules to solve tasks like renaming files or generating random lists of experimental stimuli. On top of all that, you know how to use external packages (and more importantly to solve weird annoying errors installing external packages), program a simple experiment and how to import, process and analyze experimental data. \n",
    "\n",
    "Now let's put that into practice! For this final homework assignment we like you to read in a dataset of choice. We ask you to create an analysis-notebook, that should contain at least:\n",
    "\n",
    "- A nice introduction at the top, explaining what you did (in a Markdown cell)\n",
    "- A table summarizing your data\n",
    "- A plot of your contrast of interest\n",
    "- A statistical test to determine its significance\n",
    "- In between: text boxes in markdown describing what you are doing\n",
    "\n",
    "\n",
    "We like you to do this in a completely new, fresh notebook. If you want, you can also use one of your own datasets! \n",
    "\n",
    "\n",
    "Here we'd also like to thank you for your great efforts during the course. We really had a great time and hope you learned something useful :) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
